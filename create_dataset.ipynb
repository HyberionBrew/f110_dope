{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ORL dataset\n",
    "\n",
    "If you want you can skip this step, I already processed a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this to see how different agents behave. Running it outside jupyter notebooks might be preferable :)\n",
    "\n",
    "Regarding agents, currently its only possible to run the FTG agents, because I updated this recently, the other agents (like RL) will follow ASAP.\n",
    "\n",
    "The F110 rendering mode is human_fast, so not 1:1 realtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reward_config/progress.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/msc/f110_dope/ws_release/collect_rollouts.py:143\u001b[0m\n\u001b[1;32m    141\u001b[0m                     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/msc/f110_dope/ws_release/collect_rollouts.py:40\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(args):\n\u001b[0;32m---> 40\u001b[0m     reward_config \u001b[38;5;241m=\u001b[39m \u001b[43mRewardConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     agents \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m     42\u001b[0m     model \u001b[38;5;241m=\u001b[39m agents\u001b[38;5;241m.\u001b[39mload(args\u001b[38;5;241m.\u001b[39magent_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/release/lib/python3.8/site-packages/f110_orl_dataset/config_new.py:6\u001b[0m, in \u001b[0;36mConfig.__init__\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath \u001b[38;5;241m=\u001b[39m filepath\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/release/lib/python3.8/site-packages/f110_orl_dataset/config_new.py:9\u001b[0m, in \u001b[0;36mConfig.load_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     10\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogress_reward_weight \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprogress_reward_weight\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reward_config/progress.json'"
     ]
    }
   ],
   "source": [
    "%run collect_rollouts.py --agent_config=agent_configs/ftg_fast.json --reward_config=reward_configs/progress.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to collect a larger dataset, we have to add the parameters: `--norender` and `--record`. With the `--dataset` parameter we can specify where we want to save the rollouts (the folder must already exist!) \n",
    "\n",
    "The below script basically does this and runs collect_rollouts for all values that are contained in the folder of agent configs that is passed to it. You can check the progress of each rollout in the logs_parallel folder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agent_configs/ftg_fast.json', 'agent_configs/ftg_slow_5.json', 'agent_configs/ftg_slow.json', 'agent_configs/ftg_fast_5.json', 'agent_configs/ftg_faster_5.json', 'agent_configs/ftg_slower.json', 'agent_configs/ftg_slower_5.json', 'agent_configs/ftg_faster.json']\n"
     ]
    }
   ],
   "source": [
    "%run collect_multiple_rollouts.py --agent_config_folder=agent_configs/ --reward_config=reward_configs/progress.json --num_workers=4 --dataset_folder=datasets_new --timesteps_per_agent=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now should have a folder called `datasets_new` or whatever you called it above. This folder contains the raw numpy data for each of our agents. We have to further process this by using the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files: ['StochasticContinousFTGAgent_0.5_2_0.2_0.3_2.0', 'StochasticContinousFTGAgent_5.0_5_0.2_0.3_2.0', 'StochasticContinousFTGAgent_5.0_2_0.2_0.3_2.0', 'StochasticContinousFTGAgent_3.0_2_0.2_0.3_2.0', 'StochasticContinousFTGAgent_1.0_5_0.2_0.3_2.0', 'StochasticContinousFTGAgent_1.0_2_0.2_0.3_2.0', 'StochasticContinousFTGAgent_3.0_5_0.2_0.3_2.0', 'StochasticContinousFTGAgent_0.5_5_0.2_0.3_2.0']\n",
      "Processing StochasticContinousFTGAgent_0.5_2_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_0.5_2_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_5.0_5_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_5.0_5_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_5.0_5_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_5.0_2_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_5.0_2_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_5.0_2_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_3.0_2_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_3.0_2_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_3.0_2_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_1.0_5_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_1.0_5_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_1.0_5_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_1.0_2_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_1.0_2_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_1.0_2_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_3.0_5_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_3.0_5_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_3.0_5_0.2_0.3_2.0\n",
      "Processing StochasticContinousFTGAgent_0.5_5_0.2_0.3_2.0\n",
      "Done loading data\n",
      "Number of timesteps: 500\n",
      "StochasticContinousFTGAgent_0.5_5_0.2_0.3_2.0\n",
      "StochasticContinousFTGAgent_0.5_5_0.2_0.3_2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabian/miniconda3/envs/release/lib/python3.8/site-packages/zarr/creation.py:295: UserWarning: ignoring keyword argument 'maxshape'\n",
      "  warn(\"ignoring keyword argument %r\" % k)\n"
     ]
    }
   ],
   "source": [
    "%run process_datasets.py --input_folder=datasets_new --noappend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives you a .zarr file, which is the dataset used in the OPE implementation. There is already a precreated dataset called datasets_1412.zarr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "release",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
